{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Classifier w/ PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to do a few things here:\n",
    "\n",
    "- We want to train a model to identify digits for the individual sudoku cell image that the Recognizer has prepared.\n",
    "- We want to use this as a Tensorflow Model in the C++\n",
    "\n",
    "So what are the steps?\n",
    "\n",
    "1. First train the sudoku digit classifier (aka SudokuNet) model using the MNIST dataset\n",
    "2. Save the model to the /models folder as a .pb file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff54fae9e50>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "lr_step_gamma = 0.7\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./dataset/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./dataset/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./dataset/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# prep training dataset\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    './dataset', train=True, \n",
    "    download=True,\n",
    "    transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            (0.1307,), (0.3081,))\n",
    "    ]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, \n",
    "    batch_size=batch_size_train, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# prep test dataset\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    './dataset', \n",
    "    train=False, \n",
    "    download=True,\n",
    "    transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            (0.1307,), (0.3081,))\n",
    "    ]))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size_test, \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SudokuNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SudokuNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "      output = model(data)\n",
    "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  test_losses.append(test_loss)\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "def train(model, epoch, optimizer):\n",
    "  model.train()\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_idx % log_interval == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        100. * batch_idx / len(train_loader), loss.item()))\n",
    "      train_losses.append(loss.item())\n",
    "      train_counter.append(\n",
    "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "      torch.save(model.state_dict(), './models/model.pth')\n",
    "      torch.save(optimizer.state_dict(), './models/optimizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.304408\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.315722\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.297674\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.292122\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.303820\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.308620\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.295024\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.317907\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.308458\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.302550\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.281121\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.304714\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.295491\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.289749\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.289761\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.306260\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.282298\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 2.298188\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.289312\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 2.299911\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.313119\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 2.290167\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 2.294592\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 2.308033\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 2.315995\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 2.299022\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 2.311225\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 2.311539\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 2.311274\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 2.294774\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.294817\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 2.309743\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 2.304499\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 2.299244\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 2.304790\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 2.298543\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 2.313777\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 2.290372\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 2.292181\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 2.303107\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.291165\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 2.294806\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 2.285684\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 2.313217\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 2.290626\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 2.297372\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 2.300030\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 2.290397\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 2.303180\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 2.296028\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.310538\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 2.308940\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 2.302868\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 2.297346\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 2.287656\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 2.292857\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 2.322602\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 2.271069\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 2.295210\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 2.295861\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.315013\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 2.307900\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 2.300086\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 2.307588\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 2.295166\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 2.300915\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 2.310639\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 2.306729\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 2.304707\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 2.303517\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.300166\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 2.301809\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 2.314350\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 2.311732\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 2.287638\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 2.299884\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 2.304853\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 2.309000\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 2.308637\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 2.301435\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.302062\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 2.308306\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 2.310891\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 2.317840\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 2.295528\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 2.293439\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 2.295644\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 2.312501\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 2.302737\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 2.304658\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 2.305468\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 2.301706\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 2.296194\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 2.303464\n",
      "\n",
      "Test set: Avg. loss: 2.3021, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.314951\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 2.284637\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 2.312203\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 2.298693\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 2.307926\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 2.300288\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 2.296865\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 2.315820\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 2.298723\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 2.299441\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 2.300371\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 2.300588\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 2.295689\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 2.308902\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 2.308499\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 2.295581\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 2.312566\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 2.304652\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 2.285002\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 2.311941\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 2.295141\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 2.313046\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 2.308888\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 2.303546\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 2.296195\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 2.296043\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 2.292940\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 2.296627\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 2.296493\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 2.294480\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 2.295666\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 2.305048\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 2.306166\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 2.301055\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 2.304604\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 2.313867\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 2.305204\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 2.308089\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 2.296783\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 2.305663\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 2.305248\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 2.298357\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 2.301288\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 2.302277\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 2.308195\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 2.309244\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 2.305198\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 2.292780\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 2.299194\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 2.306781\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 2.301083\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 2.297552\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 2.295567\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 2.293367\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 2.299152\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 2.298761\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 2.305245\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 2.285371\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 2.301652\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 2.303820\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 2.296086\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 2.303366\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 2.295710\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 2.305398\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 2.288452\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 2.322663\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 2.307417\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 2.294614\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 2.306078\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 2.311902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 2.312710\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 2.314729\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 2.301115\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 2.317662\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 2.311439\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 2.301976\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 2.316561\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 2.298466\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 2.294756\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 2.290611\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 2.314969\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 2.305789\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 2.307512\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 2.298409\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 2.307837\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 2.309788\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 2.309898\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 2.309121\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 2.292130\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 2.296915\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 2.295523\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 2.307430\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 2.297608\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 2.318692\n",
      "\n",
      "Test set: Avg. loss: 2.3014, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.303646\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 2.305455\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 2.309653\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 2.301643\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 2.310532\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 2.305715\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 2.288005\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 2.299742\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 2.313042\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 2.292253\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 2.302479\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 2.291639\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 2.289389\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 2.301915\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 2.305182\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 2.306628\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 2.305779\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 2.314815\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 2.310719\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 2.285962\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 2.304818\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 2.300979\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 2.306628\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 2.291865\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 2.308350\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 2.295978\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 2.300240\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 2.295743\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 2.295987\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 2.300234\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 2.303614\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 2.303744\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 2.286006\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 2.308944\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 2.302234\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 2.297019\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 2.292925\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 2.308287\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 2.285972\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 2.296850\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 2.294605\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 2.314564\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 2.302271\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 2.302124\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 2.308118\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 2.297606\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 2.305160\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 2.296306\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 2.295565\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 2.287077\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 2.303967\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 2.297746\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 2.287999\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 2.302260\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 2.301899\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 2.299791\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 2.303283\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 2.305939\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 2.299070\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 2.295265\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 2.298599\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 2.295896\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 2.293295\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 2.309787\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 2.295558\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 2.305125\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 2.290833\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 2.302857\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 2.302671\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 2.303595\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 2.304559\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 2.296450\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 2.306993\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 2.309525\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 2.306839\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 2.306586\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 2.299510\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 2.298886\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 2.306004\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 2.297885\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 2.301154\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 2.319792\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 2.305401\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 2.298361\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 2.310886\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 2.305663\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 2.300326\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 2.297457\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 2.302153\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 2.298043\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 2.290678\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 2.305602\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 2.304649\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 2.301968\n",
      "\n",
      "Test set: Avg. loss: 2.3013, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.306298\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 2.316357\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 2.295531\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 2.305254\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 2.295040\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 2.296840\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 2.312610\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 2.297658\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 2.302092\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 2.311607\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 2.295005\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 2.292521\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 2.305914\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 2.303893\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 2.294361\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 2.296686\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 2.311937\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 2.300183\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 2.295157\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 2.306066\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 2.311368\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 2.297959\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 2.305077\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 2.296475\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 2.302913\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 2.292323\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 2.307587\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 2.304579\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 2.284770\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 2.306464\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 2.301446\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 2.296170\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 2.286335\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 2.300009\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 2.301090\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 2.318226\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 2.305070\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 2.298264\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 2.312606\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 2.295456\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 2.303294\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 2.309293\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 2.322826\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 2.306194\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 2.310629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 2.292940\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 2.304184\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 2.305822\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 2.306401\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 2.306067\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 2.303639\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 2.291969\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 2.300966\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 2.303765\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 2.296005\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 2.292669\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 2.295856\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 2.290114\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 2.291241\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 2.291286\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 2.304090\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 2.303486\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 2.301451\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 2.300151\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 2.315921\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 2.300960\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 2.290843\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 2.304539\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 2.316332\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 2.290903\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 2.288038\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 2.300431\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 2.298494\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 2.307282\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 2.311106\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 2.304192\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 2.287056\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 2.302981\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 2.307691\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 2.300444\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 2.307361\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 2.305694\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 2.322221\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 2.311320\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 2.296430\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 2.303853\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 2.313877\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 2.299357\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 2.308858\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 2.297897\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 2.307734\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 2.307285\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 2.309487\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 2.297255\n",
      "\n",
      "Test set: Avg. loss: 2.3011, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.301145\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 2.296101\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 2.297712\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 2.291595\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 2.289442\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 2.291642\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 2.307863\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 2.315513\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 2.306566\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 2.295666\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 2.303856\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 2.296802\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 2.310053\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 2.307298\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 2.305429\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 2.298712\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 2.305414\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 2.298449\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 2.294070\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 2.300416\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 2.299447\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 2.293673\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 2.294585\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 2.298600\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 2.301110\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 2.300371\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 2.293828\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 2.296833\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 2.309809\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 2.299909\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 2.299455\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 2.296553\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 2.308123\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 2.295911\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 2.300835\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 2.291132\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 2.306638\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 2.294856\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 2.309883\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 2.291899\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 2.292130\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 2.301880\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 2.301567\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 2.310346\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 2.290918\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 2.293618\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 2.298800\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 2.299591\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 2.301634\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 2.308253\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 2.305285\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 2.284907\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 2.300312\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 2.300498\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 2.308302\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 2.306691\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 2.320472\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 2.294562\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 2.305995\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 2.293231\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 2.304841\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 2.295760\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 2.303958\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 2.290907\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 2.304127\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 2.279624\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 2.293747\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 2.300172\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 2.297507\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 2.305588\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 2.307876\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 2.293708\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 2.297735\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 2.301700\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 2.311983\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 2.304508\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 2.288926\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 2.307658\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 2.298614\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 2.299750\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 2.303703\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 2.294922\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 2.286951\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 2.303714\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 2.306427\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 2.301389\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 2.298743\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 2.301005\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 2.291644\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 2.299293\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 2.299041\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 2.296927\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 2.294688\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 2.309341\n",
      "\n",
      "Test set: Avg. loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = SudokuNet()\n",
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=lr_step_gamma)\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(model, epoch, optimizer)\n",
    "    test(model)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV: Evaluate performance & further training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(train_counter, train_losses, color='blue')\n",
    "plt.scatter(test_counter, test_losses, color='red')\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('negative log likelihood loss')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  output = model(example_data)\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Prediction: {}\".format(\n",
    "    output.data.max(1, keepdim=True)[1][i].item()))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continued training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continued_model = SudokuNet()\n",
    "continued_optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = torch.load(\"./models/model.pth\")\n",
    "continued_network.load_state_dict(model_state_dict)\n",
    "\n",
    "optimizer_state_dict = torch.load(\"./models/optimizer.pth\")\n",
    "continued_optimizer.load_state_dict(optimizer_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4,9):\n",
    "  test_counter.append(i*len(train_loader.dataset))\n",
    "  train(i)\n",
    "  test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(train_counter, train_losses, color='blue')\n",
    "plt.scatter(test_counter, test_losses, color='red')\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('negative log likelihood loss')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep to save the model to be used in C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_image, example_label = next(iter(train_loader))\n",
    "# run the tracing\n",
    "traced_script_module = torch.jit.trace(model, example_image)\n",
    "# save the converted model\n",
    "traced_script_module.save(\"./models/converted_model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
